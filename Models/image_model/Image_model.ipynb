{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18b6031c-ebca-407c-860c-0ab016949f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Define the paths to your dataset directories\n",
    "dataset_dir = 'dataset'  # Update this path to your dataset\n",
    "real_dir = os.path.join(dataset_dir, 'real')\n",
    "default_dir = os.path.join(dataset_dir, 'default')\n",
    "fake_dir = os.path.join(dataset_dir, 'fake')\n",
    "\n",
    "# Initialize lists to hold image data and labels\n",
    "train_images = []\n",
    "train_labels = []\n",
    "\n",
    "# Define a function to load and preprocess the image\n",
    "def load_and_preprocess_image(img_path):\n",
    "    img = image.load_img(img_path, target_size=(224, 224))  # Resize to 224x224\n",
    "    img_array = image.img_to_array(img)  # Convert image to array\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Add an extra dimension for batch size\n",
    "    img_array = preprocess_input(img_array)  # Normalize the image\n",
    "    return img_array\n",
    "\n",
    "# Define a function to load images and assign labels\n",
    "def load_images_from_directory(directory, label):\n",
    "    for img_file in os.listdir(directory):\n",
    "        img_path = os.path.join(directory, img_file)\n",
    "        # Check if the path is a file and has an appropriate image extension\n",
    "        if os.path.isfile(img_path) and img_path.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp')):\n",
    "            try:\n",
    "                img = load_and_preprocess_image(img_path)  # Load and preprocess the image\n",
    "                train_images.append(img)  # Append the preprocessed image to the list\n",
    "                train_labels.append(label)  # Append the corresponding label (integer)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading image {img_path}: {e}\")  # Print the error message\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e7b5afe-e4a4-488f-a32b-b2ff2f4019eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 126 images belonging to 3 classes.\n",
      "Found 158 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Define batch size and directories\n",
    "batch_size = 16\n",
    "train_dir = 'dataset/train/'\n",
    "validation_dir = 'dataset/validation/'\n",
    "augmented_train_dir = 'dataset/augmented_train/'  # Directory to save augmented images\n",
    "\n",
    "# Create the directory to save augmented images if it doesn't exist\n",
    "if not os.path.exists(augmented_train_dir):\n",
    "    os.makedirs(augmented_train_dir)\n",
    "\n",
    "# Create an instance of ImageDataGenerator with augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Validation data should only rescale\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Create generators\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    save_to_dir=augmented_train_dir,  # Save augmented images here\n",
    "    save_prefix='aug',  # Prefix for saved images\n",
    "    save_format='jpeg'  # Format for saved images\n",
    ")\n",
    "\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efa00725-aeb9-4098-a51b-8513a9ec3a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
      "\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 2us/step\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shriv\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 298ms/step - accuracy: 0.2083 - loss: 6.0053"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shriv\\AppData\\Local\\Programs\\Python\\Python310\\lib\\contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 564ms/step - accuracy: 0.2592 - loss: 5.8714 - val_accuracy: 0.6835 - val_loss: 5.0932\n",
      "Epoch 2/15\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 356ms/step - accuracy: 0.5880 - loss: 5.1292 - val_accuracy: 0.7278 - val_loss: 4.8607\n",
      "Epoch 3/15\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 344ms/step - accuracy: 0.6518 - loss: 4.8944 - val_accuracy: 0.7278 - val_loss: 4.6994\n",
      "Epoch 4/15\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 338ms/step - accuracy: 0.7611 - loss: 4.7040 - val_accuracy: 0.7405 - val_loss: 4.6202\n",
      "Epoch 5/15\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 322ms/step - accuracy: 0.7301 - loss: 4.6242 - val_accuracy: 0.7468 - val_loss: 4.5637\n",
      "Epoch 6/15\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 332ms/step - accuracy: 0.7972 - loss: 4.5062 - val_accuracy: 0.7658 - val_loss: 4.5183\n",
      "Epoch 7/15\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 325ms/step - accuracy: 0.8322 - loss: 4.4124 - val_accuracy: 0.7911 - val_loss: 4.4696\n",
      "Epoch 8/15\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 324ms/step - accuracy: 0.8276 - loss: 4.3527 - val_accuracy: 0.7722 - val_loss: 4.4327\n",
      "Epoch 9/15\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 321ms/step - accuracy: 0.8924 - loss: 4.2658 - val_accuracy: 0.7722 - val_loss: 4.4169\n",
      "Epoch 10/15\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 323ms/step - accuracy: 0.8462 - loss: 4.2337 - val_accuracy: 0.7722 - val_loss: 4.3641\n",
      "Epoch 11/15\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 321ms/step - accuracy: 0.8869 - loss: 4.2256 - val_accuracy: 0.7975 - val_loss: 4.3128\n",
      "Epoch 12/15\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 323ms/step - accuracy: 0.8430 - loss: 4.2297 - val_accuracy: 0.8038 - val_loss: 4.2765\n",
      "Epoch 13/15\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 323ms/step - accuracy: 0.8960 - loss: 4.1100 - val_accuracy: 0.8038 - val_loss: 4.2442\n",
      "Epoch 14/15\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 311ms/step - accuracy: 0.9295 - loss: 4.0281 - val_accuracy: 0.8038 - val_loss: 4.2112\n",
      "Epoch 15/15\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 323ms/step - accuracy: 0.9319 - loss: 3.9683 - val_accuracy: 0.8165 - val_loss: 4.1748\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x17f655486d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.utils import class_weight\n",
    "import numpy as np\n",
    "\n",
    "# Load the pre-trained MobileNetV2 model\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Freeze the base model layers to prevent them from being trained\n",
    "base_model.trainable = False\n",
    "\n",
    "# Create model with regularization and dropout\n",
    "model = tf.keras.Sequential([\n",
    "    base_model,\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    tf.keras.layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01)),  # Reduced dense layer size\n",
    "    tf.keras.layers.Dropout(0.5),  # Adjusted dropout rate\n",
    "    tf.keras.layers.Dense(3, activation='softmax')  # 3 classes: Real, Default, Suspicious\n",
    "])\n",
    "\n",
    "# Set a lower learning rate\n",
    "learning_rate = 0.0001  # Adjust as necessary\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "train_images_count = 126\n",
    "validation_images_count = 158 \n",
    "batch_size = 8\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(train_generator.classes),\n",
    "    y=train_generator.classes\n",
    ")\n",
    "\n",
    "# Convert class weights to a dictionary\n",
    "class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "\n",
    "# Fit the model with class weights\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_images_count // batch_size,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_images_count // batch_size,\n",
    "    epochs=15,\n",
    "    class_weight=class_weight_dict  # Pass the class weights here\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82e56379-f54a-4215-9a3b-da7cb8a047dc",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '_.aishvarya_profile_pic.jpg.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[0;32m     14\u001b[0m profile_pic \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_.aishvarya_profile_pic.jpg.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 15\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_profile_picture\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprofile_pic\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe profile picture is classified as: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m, in \u001b[0;36mpredict_profile_picture\u001b[1;34m(img_path)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_profile_picture\u001b[39m(img_path):\n\u001b[1;32m----> 2\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mload_and_preprocess_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(img)\n\u001b[0;32m      4\u001b[0m     class_idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(predictions, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n",
      "Cell \u001b[1;32mIn[1], line 19\u001b[0m, in \u001b[0;36mload_and_preprocess_image\u001b[1;34m(img_path)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_and_preprocess_image\u001b[39m(img_path):\n\u001b[1;32m---> 19\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_img\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Resize to 224x224\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     img_array \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mimg_to_array(img)  \u001b[38;5;66;03m# Convert image to array\u001b[39;00m\n\u001b[0;32m     21\u001b[0m     img_array \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(img_array, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Add an extra dimension for batch size\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\shriv\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\image_utils.py:235\u001b[0m, in \u001b[0;36mload_img\u001b[1;34m(path, color_mode, target_size, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, pathlib\u001b[38;5;241m.\u001b[39mPath):\n\u001b[0;32m    234\u001b[0m         path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(path\u001b[38;5;241m.\u001b[39mresolve())\n\u001b[1;32m--> 235\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    236\u001b[0m         img \u001b[38;5;241m=\u001b[39m pil_image\u001b[38;5;241m.\u001b[39mopen(io\u001b[38;5;241m.\u001b[39mBytesIO(f\u001b[38;5;241m.\u001b[39mread()))\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '_.aishvarya_profile_pic.jpg.jpg'"
     ]
    }
   ],
   "source": [
    "\n",
    "def predict_profile_picture(img_path):\n",
    "    img = load_and_preprocess_image(img_path)\n",
    "    predictions = model.predict(img)\n",
    "    class_idx = np.argmax(predictions, axis=1)[0]\n",
    "    \n",
    "    if class_idx == 0:\n",
    "        return 'default'\n",
    "    elif class_idx == 1:\n",
    "        return 'fake'\n",
    "    else:\n",
    "        return 'Real'\n",
    "\n",
    "# Example usage:\n",
    "profile_pic = '_.aishvarya_profile_pic.jpg.jpg'\n",
    "label = predict_profile_picture(profile_pic)\n",
    "print(f\"The profile picture is classified as: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af74eb18-37d0-4cfa-9108-3671e07687c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'default': 0, 'fake': 1, 'real': 2}\n",
      "{'default': 0, 'fake': 1, 'real': 2}\n"
     ]
    }
   ],
   "source": [
    "print(train_generator.class_indices)\n",
    "print(validation_generator.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2557549d-10f3-41b0-bfc1-9e1ac915a03c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
